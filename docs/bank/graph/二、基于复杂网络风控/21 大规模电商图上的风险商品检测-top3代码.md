# 大规模电商图上的风险商品检测-top3代码

[小伍哥聊风控](javascript:void(0);) *2023-01-28 21:16* *浙江*

以下文章来源于Coggle数据科学 ，作者SYSU-GEAR

<img src="http://wx.qlogo.cn/mmhead/Q3auHgzwzM6KicerJ0BHX6hbUMOg1rNdT1tw8lxR0bk6j5BOmmPen8A/0" alt="img" style="zoom:50%;" />

**Coggle数据科学**.

Coggle全称Communication For Kaggle，专注数据科学领域竞赛相关资讯分享。

## **赛题背景分析**

本次比赛为大规模电商图上的风险商品检测，挑战者需要利用来源于真实场景中的大规模的、异构的、存在噪声和缺失的图数据，以及极不均衡的样本，进行风险商品检测。主要挑战为：

- 图规模巨大且异构（13M个节点，157M条边，7种节点类型，7种边类型）
- 模型Inductive推理能力以及迁移泛化能力（复赛图数据与初赛完全不同）
- 黑白样本严重不均衡（训练集正负样本比例接近1:10）
- 数据噪声（节点特征、边关系可能存在噪声）

我们在比赛过程中，针对上述挑战，提出了 HeteroGNN 解决方案，在初赛和复赛分别取得了第8和第3的成绩。

**代码地址：**EdisonLeeeee/ICDM2022_competition_3rd_place_solution: 3rd place solution of ICDM 2022 Risk Commodities Detection on Large-Scale E-Commence Graphs (github.com)

**比赛链接：**https://tianchi.aliyun.com/competition/entrance/531976/introduction?spm=5176.21852664.0.0.3acd2527elmP4G



## **解决方案: HeteroGNN**

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpbeFtCQNG7rwnHD8PGibdCQs5yZC2kde6FXt1tOLsKkumzX2yE3vxPeQ/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

图 1. HeteroGNN 整体框架图。本方案采用mini-batch训练方式，针对输入的异构图数据以及训练节点，首先采样节点的两跳邻居，然后经过DropMetapath的操作随机丢弃预先设定好的Metapath，将原本的图划分成为Masked Graph和Remaining Graph。接着，将Remaining Graph输入模型，分别对每种边类型采用一层GraphConv进行建模学习节点表征，再将同类型节点表征Group聚合。最后，将得到的item embedding利用Jumping Knowledge进行Node decoding得到预测的类别，同时利用Edge decoding重构Masked Graph的Metapath作为辅助损失进行优化。

本方案基于RGCN[1]与HAN[2]，将异构图看成由多种不同类型边的同构图构成的multi-view Graph。因此，我们针对每种graph view（edge type）采用不同的图卷积层学习节点表征，最后再将同种类型节点的表征进行合并。如图1所示，对每种边类型采用的图卷积层为统一的GraphConv3层。GraphConv是SAGEConv4的一个变种，它们的主要区别在于：经典的SAGEConv采用的是mean聚合方式，而GraphConv在聚合邻居信息的时候采用的是sum聚合方式。具体来说，对于每个节点ii，GraphConv的聚合公式如下所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtptWKuj7cCicouEWt8jyqDHw0ovLskXQLD30YkVzPEGIbAsSRVicxnJOgg/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

其中， 为源节点到目标节点的边权（一般为1）。相比于mean，采用sum的聚合方式能够更好的区分不同图结构信息。如图2所示，假设图中所有的边权重都为1，在聚合过程中，对于图(a)与图(b)mean的聚合方式会都会得到 1，而sum的聚合方式则分别会得到2和4。因此，sum对不同的子图结构区分能力较强，基于sum聚合方式构造的模型具有更强的表达能力。

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtph1Z0DMibJ4bmhN2HiboOZKQArMnMF6mjeCgQhPf8iascqyN3w1p29ZCUQ/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

基于不同边类型利用GraphConv构造的模型是我们方法的基本框架，下面我们将详细介绍比赛过程中解决上述挑战的各部分方法和思路分析：主要包括邻居采样、重采样、Batch Normalization、随机均匀噪声、Jumping Knowledge、Drop Metapath、Masked Label Propagation以及Masked Autoencoding。

### 邻居采样

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpWiahic2XgggRWR403RFuic0GVeREfy6gugS8VIjNTXCyaOBH3zv2UMCJA/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

邻居采样方法来源于GraphSAGE论文[4]，能够缓解图神经网络模型无法在大规模图数据上进行学习的问题。给定一批训练节点作为Batch根节点，该方法采样基于根节点的子图输入模型进行训练和推理。邻居采样通过预先设定每一层邻居的采样数目，限制了输入的子图大小，从而避免在规模庞大的全图进行计算操作，减少了内存和计算消耗，在实际工业场景中具有广泛的应用。此外，邻居采样能够使得模型具有Inductive推理的能力，十分适合应用于节点和边结构变化的场景。在本次比赛中，我们也采用了邻居采样的方法来解决图数据规模庞大难以训练以及对新场景图数据泛化能力的问题。考虑到两层图神经网络即具有较好的表现，因此我们采样的子图深度固定为2。

### 重采样

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpGyvMhRNfoYCd5vM1LSYwr8svwUEicOB0XvGccg6KcPLat1DCDAOHia4A/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpEDMj2m1nQNcSpFS134BuGRv13yXGBqFibRIl2K3Iesy0NS8ozq3MR6w/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

我们通过对训练数据观察发现，正负样本比例严重不均衡，正样本数量远远低于负样本数量。样本不均严重影响了模型的训练效果，甚至会影响到我们对模型好坏的判断，因为模型对占比比较高的类目准确率非常高，对占比很低的类目预估的偏差特别大，但是由于占比较高的类目对loss/metric影响较大，我们会认为得到了一个较优的模型。本次比赛我们采用的解决方案是重采样，通过随机对正样本过采样或者对负样本降采样，使得在每个Batch中各个正负样本的数量大致相同，加快模型训练收敛。

### Batch Normalization

Batch Normalization (BN)的作用是通过控制各层输入分布的均值和方差，稳定各层输入的分布。根据论文[5]中的结论，BN可以防止梯度爆炸或弥散、提高训练时模型对于不同超参（学习率、初始化）的鲁棒性、可以让大部分的激活函数能够远离其饱和区域。此外，使用了BN方法的神经网络模型损失和梯度能够减少抖动，使得优化损失的解空间更平滑，从而不容易受到噪声的干扰。在后续消融实验中也发现，使用了BN的模型收敛更快，鲁棒性、泛化性更好。

### 随机均匀噪声

由于原本的图数据中存在较多的噪声，直接进行训练容易导致模型过拟合噪声数据，陷入次优解空间。因此，我们在训练过程中，人为添加噪声作为数据增强以提高模型对噪声数据的鲁棒性。在训练过程中添加噪声/扰动是一种有效的对抗训练方式。在对抗训练的过程中，输入数据会被混合一些微小的扰动/噪声，然后使模型适应这种改变，从而对对抗样本和噪声具有鲁棒性。对抗训练的关键在于如何生成扰动，目前学术界最实用的方法是Goodfellow等人提出的FGSM[6]方法，其通过梯度上升的方式来生成最大化模型损失的扰动。然而，FGSM需要额外进行一次前向及一次反向传播计算梯度，计算复杂度较高。考虑到模型的可扩展性，我们采用简单的随机方式生成噪声。具体而言，我们对每种类型的输入的节点特征xx添加了如下噪声：

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpKzqs0ANANndpswsiaJanJH33ZDPicHGibj28TA4HPY2D79YwSprLEibvzw/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

其中，U(-0.5,0.5)U(−0.5,0.5)为[-0.5,0.5]的连续均匀分布。这一方法虽然简单，但在比赛过程中却取得了较好的效果。

### Jumping Knowledge

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpNQCjIiaoymRlprwIJoE7cORRPbd3GcZP9tLv20N5y74uGUCxicM8txpg/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

Jumping Knowledge (JK)这一方法出自论文[7]。文章指出：虽然图神经网络的消息聚合方式能够适应不同结构的图，但是其固定的层级结构以及聚合邻居节点的信息传播方式会给不同邻域结构的节点表达带来比较大的偏差，而这种偏差对于不同节点来说是灾难性的。例如，有的节点自身特征信息丰富，不需要邻域特征即可达到较好的分类效果，而有的节点则需要依赖高阶邻域信息才能够取得较好的表现，简单采用较浅或者较深的网络会对不同类型的节点性能带来严重影响。考虑到本次比赛的数据中存在一定程度的噪声干扰——即节点邻域内的某些邻居可能会对下游分类任务产生干扰，因此，我们在对目标节点进行最终预测时，采用了JK+MLP结构，同时考虑了原始输入特征以及每一层聚合之后的特征，在增加节点特征信息量的同时，对噪声也有一定的鲁棒性。具体实现方式如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtptibXps9WhRSnjTl4b6ChfaMHYsQM1bgA5Qp8Fv3eP16fiaNibQRVqYUQQ/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

其中，代表第层的节点特征且为初试输入特征；为拼接操作。MLP为多层感知机网络，作为节点级别的Node decoder来提取最终节点特征。在初赛中，MLP设计为简单的单层前向网络，即MLP(*x*)=Linear(*x*)；在复赛阶段，我们提高了MLP的复杂度，使得学得的模型具有更强的泛化能力：

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpuWGU0Bgzw1iboUjbPu6ibffnLFqf1jb3EQfbu4RL03lWxAxRk2Zv2ibAg/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

其中，\text{Linear}(x)=Wx+bLinear(*x*)=*W**x*+*b*为单层线性网络。通过JK+MLP的方案，可以使得节点最后的表达能够融合进不同层的信息，减少噪声干扰，提高模型对不同邻居结构节点的适应性。

### DropMetapath

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpuzjbCXGVCu0xXBGkKnVAOaUE6ZAY2hANYoYrEMSG1IAic7c3sDvJjHQ/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

受到论文DropEdge[8]与MaskGAE[9]的启发，我们提出了DropMetapath，一种面向异构图数据的结构化Dropout技巧。Drop Metapath 具有类似于DropEdge和Dropout的作用，通过在训练过程中随机丢弃一定数量的Metapath，从而减少模型过拟合风险，提高模型泛化能力。在训练阶段，Drop Metapath能够产生输入图数据的不同随机变形，这可以被认为是一种针对于图的数据增强方法。同时，我们使用随机的邻居子集进行聚合而不是使用所有的邻居，能够避免模型对于过度依赖某些特定的Metapath而忽视了其它图结构信息。

DropMetapath方法的重点在于如何选取有效的Metapath在训练阶段进行丢弃。根据对异构图中不同类型节点之间的边关系分析发现，最终分类的商品item主要与类型为ff的节点连接，而类型为ff的节点则位于所有关系图的中心——存在多种类型的节点与其相连并且是重要“枢纽”节点，这充分说明了类型为ff的节点在消息传递过程中的重要性。

在训练过程中，我们首先采样Metapath中一定数量的起始类型节点（如50%），利用这些节点作为起点进行步长为1的Random Walk采样下一跳的节点。接着，基于采样的节点进行下一种类型节点的Random Walk采样，直至完成整条Metapath。最后，将采样过程中所有的Metapath从图中删除，利用剩余的图结构信息进行学习。

### Masked Label Propagation (初赛使用)

受到UniMP[10]方法的的启发，我们在比赛中引入Masked Label Propagation。首先，基于Label Propagation，对于有标签的类型节点item，我们在训练过程中将节点标签经过Embedding层融入消息传播中：

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpF9XgsfDGIxdG3xHrNJia7ibYdMQmyxBfUN9uL5z2ia2GDnAUoqNN4dfcw/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)



其中为输入商品节点的特征。上述方式可以看作是图神经网络在消息传播过程中同时传递节点特征信息与标签信息。然而，直接将标签的Embedding添加到子图中的所有节点容易造成标签信息泄露，从而使得模型过拟合标签信息。因此，我们对在一个Batch子图中，对根节点的标签进行了Mask操作。如图所示，在传播过程中只对邻居节点的标签进行传播，从而避免了训练节点标签泄露的问题。

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpEudBSFMBXQWHNIBNo9J60HdGP2ppyJNEdH5XE6KZy3mcPBicA3zB25Q/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

在测试阶段，我们不对节点进行Mask操作，将所有训练集节点标签融入消息传播中

### Masked Autoencoding (复赛使用)

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpsjdjPrd2fADxiaYHhHiaW6MSnCl5qDzrUakLkmc2Qy52vWyrvoWO0HpA/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

在上述使用的DropMetapath方法中，我们通过随机丢弃Metapath可以得到由所有丢弃的Metapath构成的一个掩模子图（Masked Graph）以及一个剩余子图（Remaining Graph） ，且。在训练过程中我们仅仅利用进行学习，并没有利用到的信息。基于近期的工作MaskGAE[9]启发，我们提出面向异构图的Masked Autoencoding 自监督学习方法，通过利用学得的节点表征去重构来辅助模型训练。

具体而言，重构的过程可以建模为链路预测任务，相比于传统Autoencoding（如GAE[11]）直接利用全图GG作为输入并重构，我们提出的Masked Autoencoding，即利用进行重构 ，具有如下优势：

- 由于和为的子集，因此有效减少了训练过程中的复杂度（包括基于的前向传播和的重构损失计算）
- 基于MaskGAE9的结论：GAE的Autoencoding本质上是一种对比学习范式，对输入进行Mask然后重建能够减少子图之间的重叠，从而促进对比学习训练。如图所示，Masked Autoencoding的学习范式能够显著促进GAE的对比学习能力。

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtp9vJtfISsJRDnWqn0eygrVUEf0LTiaKyURh8LL0baS9PCVZtDrnhYtyQ/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

（a）GAE本质上是对一条边两端节点构成的子图进行对比学习训练，但子图之间往往存在着较大的重叠，导致对比学习效果不佳；（b）通过对输入进行Mask然后预测，可以显著减少子图之间的重叠度，构造差异性样本促进对比学习训练。



## 

## **模型消融研究**

### 初赛阶段

注：+为增加模块/方法

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpE02CtCCOLIXeHtVer4A2EmPzzCp5v4viayEtTBsH5BJ34OA7piakHlQw/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

从上述消融实验，可以得到如下结论：

- 添加随机均匀噪声和Masked Label Propagation在验证集上效果提升最为明显。但在线上测试集上，提升效果最多的是Masked Label Propagation和重采样。
- 增加重采样在验证集效果降低了，但是在测试集上提升较多，说明基于类别均衡数据训练得到的模型具有较好的泛化性。
- 增加Jumping Knowledge 和 Drop Metapath虽然相较于模型4验证集效果降低，但是在测试集上稳定提升。推测可能是测试集上的噪声相较于验证集更为严重，而增加上述两个方法能够提高模型在噪声数据下的鲁棒性。
- Masked Label Propagation 能够显著提升训练集和测试集的性能，这是由于我们将标签传播融入消息传播中，确保具有Ground-truth的节点不会受到噪声干扰的同时，辅助邻域节点的预测。

### 复赛阶段

注：-为去掉模块/方法

![图片](https://mmbiz.qpic.cn/mmbiz_png/uoTGEibAZUEj5QKNJaoRGDXTRD2MWMibtpSYd2ABtOR96VibdNQGlon7nibQJ92MUBUicw0jbaXSfbow70RhSjtZkzQ/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

从上述消融实验，可以得到如下结论：

- 初赛最优模型，即使用了Masked Label Propagation 的方法在复赛数据集效果较差。这是由于复赛数据集中并没有节点标签信息，无法利用标签传播引导模型预测，甚至还会由于缺少了标签传播部分导致模型性能降低。在去掉Masked Label Propagation 之后 (序号1)，模型在测试集上的性能反而提升了。
- 复赛阶段考查的是模型对于新场景/领域数据的泛化能力。在复赛阶段，我们尝试提高了隐藏层大小以及Node decoder的层数，可以发现尽管本地验证集效果并未得到明显提升，但是线上测试集效果却得到了显著提升。说明在一定范围内，复杂的模型（如隐藏层大小较大，深度较深）比简单的模型泛化能力较强。
- 除了提高模型复杂度，对于性能增益最大的是采用的Masked Autoencoding自监督损失，说明将节点分类任务与链路预测任务结合对于模型泛化能力的提升有帮助。同时，Masked Autoencoding还能够促进模型学习图的潜在重要结构（underlying structure），这对于存在噪声的场景中尤为有效。

### 其它尝试：基于中值聚合函数的鲁棒图神经网络

在本次比赛中我们的HeteroGNN模型采用的是sum的聚合方式。在前面的分析中，sum的聚合方式具有比mean的方式更强的区分图结构能力，在比赛中也取得了较好的结果。然而，在存在噪声数据的情况下，sum与mean的聚合方式都容易受到影响而输出错误的结果。在论文[11]中，作者提出使用鲁棒统计学领域的崩溃点（breakdown point）理论进行分析图神经网络聚合函数的鲁棒性。崩溃点（breakdown point）理论就是衡量一个函数在受到数据扰动情况下鲁棒性情况。崩溃点（breakdown point）可以直观地理解为：数据中最少需要存在数量多少的数据点，能够使得函数的输出变化为无穷大（崩溃）。

基于崩溃点理论，可以知道sum和mean聚合函数的崩溃点都为1——在最坏情况下，只需要往数据中加入一个值为无穷的点，即可使得模型的输出变化为无穷大。为提高模型鲁棒性，论文11提出使用中值聚合函数median替代sum和mean作为图神经网络的聚合函数。在本次比赛中，我们也实现了基于median的聚合方式，但由于计算中位数的复杂度较高，不适用于大规模图数据。我们在比赛中尚未有一个较好的解决方案，因此最终并未采用，但后续可以尝试对median的计算方式进行改进以提高效率。

### 参考文献

1. ESWC 18, Modeling Relational Data with Graph Convolutional Networks ↩︎
2. WWW 19, Heterogeneous Graph Attention Network ↩︎
3. AAAI 19, Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks ↩︎
4. NeurIPS 17, Inductive Representation Learning on Large Graphs ↩︎ ↩︎
5. NeurIPS 18, How Does Batch Normalization Help Optimization? ↩︎
6. ICLR 15, Explaining and Harnessing Adversarial Examples ↩︎
7. ICML 18, Representation Learning on Graphs with Jumping Knowledge Networks ↩︎
8. ICLR 20, DropEdge: Towards Deep Graph Convolutional Networks on Node Classification ↩︎
9. arXiv 22, MaskGAE: Masked Graph Modeling Meets Graph Autoencoders ↩︎ ↩︎ ↩︎
10. NeurIPS 16, Variational Graph Auto-Encoders ↩︎
11. IJCAI 21, Understanding Structural Vulnerability in Graph Convolutional Networks 

**··· END ···**